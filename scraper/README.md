

# ğŸ§­ Pipeline Overview


```scraper/__init__.py  â†  ğŸ§  Orchestration script (driver)
   â””â”€â”€ login.py        â†  ğŸ”‘ Login to Schoology
   â””â”€â”€ fetch_events.py â†  ğŸ“¥ Fetch & parse all posts (calls scrape_page_events)
        â””â”€â”€ attachment_scraper.py â†  ğŸ“ Called inside scrape_page_events to extract attachment links
   â””â”€â”€ store.py        â†  ğŸ’¾ Save scraped events to a timestamped JSON
   â†“
process_attachments.py â†  ğŸ“„ Load that JSON, download files, extract text, enrich data
```

# ğŸ” Execution Flow
```
Step 1: Run `__init__.py`
â”œâ”€â”€ Login using login.py
â”œâ”€â”€ Call `fetch_paginated_events()` from fetch_events.py
â”‚   â””â”€â”€ Repeatedly call `scrape_page_events()`
â”‚       â”œâ”€â”€ Parse each post
â”‚       â”œâ”€â”€ Extract metadata
â”‚       â”œâ”€â”€ Extract attachments using extract_attachments(post) in attachment_scraper.py
â”‚       â””â”€â”€ Return list of event dicts
â”œâ”€â”€ Save result JSON using store.py
â””â”€â”€ Done âœ…

Step 2: Run `process_attachments.py`
â”œâ”€â”€ Load JSON saved by Step 1
â”œâ”€â”€ For each event:
â”‚   â””â”€â”€ Download each attachment
â”‚   â””â”€â”€ Extract text from `.pdf` or `.docx`
â”‚   â””â”€â”€ Add `attachment_contents` field to the event
â”œâ”€â”€ Save enriched output to a new JSON
â””â”€â”€ Done âœ…
```